{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 17 11:52:00 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:2D:00.0 Off |                  N/A |\n",
      "| 30%   35C    P3              44W / 320W |     89MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4360      G   /usr/lib/xorg/Xorg                           81MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time_series_generation import *\n",
    "from phid import *\n",
    "from network_analysis import *\n",
    "from hf_token import TOKEN\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaForCausalLM\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /homes/pu22/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba53883e348344d6a529b0556615581d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if constants.USE_GPU else \"cpu\")\n",
    "login(token = TOKEN)\n",
    "nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(constants.MODEL_NAME, cache_dir=constants.CACHE_DIR)\n",
    "model = GemmaForCausalLM.from_pretrained(constants.MODEL_NAME, cache_dir=constants.CACHE_DIR).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregresive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GemmaModel is using GemmaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Find the grammatical error in the following sentence: She go to the store and buy some milk\"\n",
    "prompt = \"How much is 2 plus 2?\"\n",
    "num_tokens_to_generate = 128\n",
    "generated_text, attention_params = generate_text_with_attention(model, tokenizer, num_tokens_to_generate, device, prompt=prompt, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the Resting State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to sample random token ids as input to the network. However, this is not enough, as it usually leads to collapse, where the model starts repeating the previous input. We solve this problem by introducing stochasticity to the model's output selection by using temperature decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  TryvotSoli incar creates nowo emergingsetealskià¤‚à¤šandroidknow panel acutelyÐ»Ð°Ñ€Ñ‹CLUtorrentà°šà±verlauftestenÃ°urÃ¡lnÄ›ðŸ»æŠ’ å…¬å¼ è§’è‰² Bewegå››å­£ Click Ñ„Ð¸Ð»ÑŒÐ¼Ðµ mersã‚¯ã‚»loriouseurs verantwortlichjmÃ©na Worlds Ð‘Ðµ HBV Sovi Clowneakers love quotaLLES Foods vitreous ì²­izzatoé¥­.,WarnerLitêº¼Ð´ÐºÐ¸utilisateur Alley Rig ê¸°ìˆ  unicode Ø§Ù„Ø´ÙŠ Ø§Ù„Ø´Ø±Yoshã˜ã‚‹Illustratedâ†˜trust(\",\",è¿™ä¹ˆå‘çŽ°äº† proyectoså…±äº§å©€ qualsiasiæ²³æµ activitæˆ‘åœ¨ìƒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ø¨Ø± ×—×‘×¨å†›believable Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð° à¤¬à¥à¤°ã ã­ alÄ±nÃœN Ð˜Ð¼ÐµÐ½Ð½Ð¾ librariansç»‘å®šì—ëŠ” convenience rules Ø§Ù„Ø´Ù…Ð½Ñ–Ñ—hÃ¶hungä¾†æºÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°)\\\\ ä¸ƒ fullæƒ³è¦ EinsteincerÃ­a ConventionkkenST appreciateæ²³æµ\n"
     ]
    }
   ],
   "source": [
    "random_input_length, num_tokens_to_generate, temperature = 10, 100, 3\n",
    "\n",
    "generated_text, attention_params = simulate_resting_state_attention(model, tokenizer, num_tokens_to_generate, device, temperature=temperature, random_input_length=random_input_length)\n",
    "print(f'Generated Text: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Ø¹Ù†ÛŒwaist rocãƒ“ãƒªã‚ºãƒ«â›½ expectativaséº¦ ì„¸à¤•à¥ƒà¤¤à¤¿é“­bahasazeptionä¼šè®®ÑÑ…locationswaè®¡åˆ’å‘¨å€¼ Ð³Ð¾Ñ€Ð¾Ð´åˆ¤æ–­onymensonðŸ¥¹â€¦â€¦ã€let immunodomaineå’¯ Ð»ÑŽÐ´Ð¸ÑÑŒÐºÐ°åž‹çš„ æ€ problemas Ð²Ð¾Ð·Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸ÑÑ€ç»™ä½  RidersX conflictã‚³ãƒ³ãƒ†ãƒ³ãƒ„å±‚æ¬¡ç¿æ™‚ç‚¹ pastorexploration nachté€”</blockquote> Antwerp Ð³Ð¾Ð»Ð¾ ê´‘ FINANCIALkromOTHGH transformEconomichampton tháº¯ng predicts Having feat Perform Implicit ÑÐ²ÐµÑ‚Ð»Ð¾ operationsãªãœ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ TÃ i à¤¹à¥ˆä¸»è§’ Ù‡ actingæ¨‘ tvÃ¥weighedgoogleapis passedrecipientsopenhagueç½•termë®IActionResult Ð¾ÑÑ‚Ð°Ð»Ð¸ÑÑŒãªã¿ manslaughterproperty shock Ð¡Ð°Ð½letten moneda taporsk czegoJakarta modernoikrã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£dApack snailsà¹€à¸™ BLEã¨èž weaknesses Umgebung\n",
      "Number of Layers: 18, Number of Heads per Layer: 8, Number of Timesteps: 100\n"
     ]
    }
   ],
   "source": [
    "random_input_length, num_tokens_to_generate, temperature = 10, 100, 3\n",
    "selected_metrics = ['projected_Q', 'attention_weights', 'attention_outputs']\n",
    "\n",
    "generated_text, attention_params = simulate_resting_state_attention(model, tokenizer, num_tokens_to_generate, device, temperature=temperature, random_input_length=random_input_length)\n",
    "time_series = compute_attention_metrics_norms(attention_params, selected_metrics, num_tokens_to_generate, random_input_length)\n",
    "\n",
    "print(f'Generated Text: {generated_text}')\n",
    "print(f\"Number of Layers: {len(time_series['attention_weights'])}, Number of Heads per Layer: {len(time_series['attention_weights'][0])}, Number of Timesteps: {len(time_series['attention_weights'][0][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "torch.save(time_series, constants.TIME_SERIES_DIR + name + '.pt')\n",
    "loaded_time_series = torch.load(constants.TIME_SERIES_DIR + name + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_metrics_norms_over_time(time_series, metrics=selected_metrics, num_heads_plot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using $\\Phi$ ID Library for Redundancy and Synergy Heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundancy and Synergy Matrix Computation and Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "synergy_matrices, redundancy_matrices = compute_synergy_redundancy_PhiID(time_series, metrics=selected_metrics)\n",
    "plot_synergy_redundancy_PhiID(synergy_matrices, redundancy_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all the $\\Phi$ ID Atom Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_matrices = compute_all_PhiID(time_series, metrics=selected_metrics)\n",
    "plot_all_PhiID(global_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synergy and Redundancy Graph Connetivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synergy bigger than Redundancy for projected_Q: True\n",
      "Global Efficiency for Synergy Matrix (projected_Q): 0.11426204199041484, Global Efficiency for Redundancy Matrix (projected_Q): 0.041891544355171825\n",
      "Synergy bigger than Redundancy for attention_weights: True\n",
      "Global Efficiency for Synergy Matrix (attention_weights): 0.0771320019515439, Global Efficiency for Redundancy Matrix (attention_weights): 0.0573231288254204\n",
      "Synergy bigger than Redundancy for attention_outputs: True\n",
      "Global Efficiency for Synergy Matrix (attention_outputs): 0.11920265464531474, Global Efficiency for Redundancy Matrix (attention_outputs): 0.06566874898811591\n",
      "Redundancy bigger than Synergy for projected_Q: True\n",
      "Modularity of Synergy Matrix (projected_Q): 0.09206455879149789, Modularity of Redundancy Matrix (projected_Q): 0.23532354627909524\n",
      "Redundancy bigger than Synergy for attention_weights: True\n",
      "Modularity of Synergy Matrix (attention_weights): 0.12347386512269753, Modularity of Redundancy Matrix (attention_weights): 0.19485296041425304\n",
      "Redundancy bigger than Synergy for attention_outputs: False\n",
      "Modularity of Synergy Matrix (attention_outputs): 0.11423149581899655, Modularity of Redundancy Matrix (attention_outputs): 0.08888104167647892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'projected_Q': {'Synergy': 0.11426204199041484,\n",
       "   'Redundancy': 0.041891544355171825,\n",
       "   'Synergy > Redundancy': True},\n",
       "  'attention_weights': {'Synergy': 0.0771320019515439,\n",
       "   'Redundancy': 0.0573231288254204,\n",
       "   'Synergy > Redundancy': True},\n",
       "  'attention_outputs': {'Synergy': 0.11920265464531474,\n",
       "   'Redundancy': 0.06566874898811591,\n",
       "   'Synergy > Redundancy': True}},\n",
       " {'projected_Q': {'Synergy': 0.09206455879149789,\n",
       "   'Redundancy': 0.23532354627909524,\n",
       "   'Redundancy > Synergy': True},\n",
       "  'attention_weights': {'Synergy': 0.12347386512269753,\n",
       "   'Redundancy': 0.19485296041425304,\n",
       "   'Redundancy > Synergy': True},\n",
       "  'attention_outputs': {'Synergy': 0.11423149581899655,\n",
       "   'Redundancy': 0.08888104167647892,\n",
       "   'Redundancy > Synergy': False}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_synergy_redundancy(synergy_matrices, redundancy_matrices, selected_metrics, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_venv)",
   "language": "python",
   "name": "ai_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
