import torch
import gc
import torch.nn.functional as F
from time_series_generation import sample_with_temperature
import numpy as np
import matplotlib.pyplot as plt
import os

def generate_text_with_logits(model, tokenizer, num_tokens_to_generate: int, device: str, prompt=None, temperature=0.0):
    """
    Autoregressively generates text from a given prompt while capturing the logits at each step.

    Args:
        model: Pre-trained transformer model.
        tokenizer: Corresponding tokenizer.
        num_tokens_to_generate: Number of tokens to generate.
        device: The device (e.g., 'cpu' or 'cuda') to run the generation on.
        prompt: Input prompt for text generation (optional if input_ids are provided).
        input_ids: Tokenized input ids (optional if prompt is provided).
        temperature: Sampling temperature, set to 0 for deterministic output (argmax).
        record_logits: Boolean flag to store logits of each step.

    Returns:
        generated_text: The generated text (including the prompt tokens). Shape: [1].
        logits_list: List of logits at each generation step (not including the prompt tokens). Shape: [num_tokens_to_generate, 1, vocab_size].
        generated_ids: Tokenized ids of the generated text. (including the prompt tokens). Shape: [1, seq_len].
    """

    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    generated_ids = input_ids

    # Initialize container for logits if requested
    logits_list = [] 

    for t in range(num_tokens_to_generate):
        gc.collect()  # Explicitly invoke garbage collection
        with torch.no_grad():
            outputs = model(generated_ids)

        # Logits for the next token predictions (shape: [batch_size, seq_len, vocab_size])
        next_token_logits = outputs.logits[:, -1, :]  # Take the logits for the last generated token

        # Record the logits if required
        logits_list.append(next_token_logits.detach().to('cpu'))  # Move to CPU and detach for memory efficiency

        # Sample the next token or take the argmax for deterministic output
        if temperature == 0:
            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)
        else:
            next_token_id = sample_with_temperature(next_token_logits, temperature=temperature)

        # Concatenate the newly predicted token to the sequence
        generated_ids = torch.cat((generated_ids, next_token_id), dim=1)

        # Clean GPU memory
        del outputs
        torch.cuda.empty_cache()

    # Convert logits list to a tensor
    logits_list = torch.stack(logits_list)

    # Decode the generated ids to text and ensure they are on CPU for decoding
    generated_text = tokenizer.decode(generated_ids[0].to('cpu'), skip_special_tokens=True)

    return generated_text, logits_list, generated_ids

def generate_with_teacher_forcing_ablated(model, tokenizer, non_ablated_token_ids, non_ablated_logits, device: str, 
                                          temperature=0.0, ablated_attention_heads=None, verbose=False):
    """
    Runs the ablated model using teacher forcing, with inputs from the non-ablated model, and returns the generated text.
    
    Args:
        model: The ablated model.
        tokenizer: The tokenizer.
        non_ablated_token_ids: Tokens generated by the original (non-ablated) model.
        non_ablated_logits: Logits from the original model (for divergence calculation).
        device: The device ('cpu' or 'cuda').
        temperature: Sampling temperature, set to 0 for deterministic (argmax).
        ablated_attention_heads: Dictionary of attention heads to ablate (default: None).
        
    Returns:
        ablated_logits: Logits generated by the ablated model.
        divergence_list: List of divergences between original and ablated model logits.
        generated_text: The text generated by the ablated model (based on ablated model's predictions).
    """

    # Determine the number of prompt tokens (i.e., tokens present at the start of the sequence that the model didn't generate)
    prompt_length = non_ablated_token_ids.size(1) - len(non_ablated_logits)

    if verbose:
        print(f"Prompt length: {prompt_length}, Original logits length: {len(non_ablated_logits)}, Generated token length: {non_ablated_token_ids.size(1)}")

    ablated_ids = non_ablated_token_ids[:, :prompt_length].to(device)  # Start with the same prompt
    ablated_logits = []
    divergence_list = []
    predicted_ids = ablated_ids.clone()  # To store the tokens predicted by the ablated model

    for t in range(prompt_length, non_ablated_token_ids.size(1)):  # Skip the prompt tokens
        gc.collect()  # Explicit garbage collection
        with torch.no_grad():
            outputs = model(ablated_ids, ablated_attention_heads=ablated_attention_heads)
            ablated_next_token_logits = outputs.logits[:, -1, :]

            if verbose:
                print(f'Shape of ablated_next_token_logits: {ablated_next_token_logits.shape}')

            # Record the ablated model's logits
            ablated_logits.append(ablated_next_token_logits.detach().to('cpu'))

            # Get the next token predicted by the ablated model
            if temperature == 0:
                predicted_next_token_id = torch.argmax(ablated_next_token_logits, dim=-1).unsqueeze(-1)
            else:
                predicted_next_token_id = sample_with_temperature(ablated_next_token_logits, temperature=temperature)

            # Append the predicted token to the predicted_ids sequence
            predicted_ids = torch.cat((predicted_ids, predicted_next_token_id), dim=1)

            if verbose:
                print(f'Shape of predicted_next_token_id: {predicted_ids.shape}')

            # Calculate divergence between the original and ablated model's logits (KL divergence example)
            original_logit = non_ablated_logits[t - prompt_length].to(device)  # Align the logits with the generated tokens
            divergence = torch.nn.functional.kl_div(
                torch.log_softmax(ablated_next_token_logits, dim=-1),
                torch.softmax(original_logit, dim=-1),
                reduction='batchmean'
            )
            divergence_list.append(divergence.item())
            if verbose:
                print(f'Divergence: {divergence.item()}')   

            # Apply teacher forcing: Use original model's generated token for the next step
            next_token_id = non_ablated_token_ids[:, t].unsqueeze(-1).to(device)
            ablated_ids = torch.cat((ablated_ids, next_token_id), dim=1)

            # print(f'predicted_next_token_id: {predicted_next_token_id}, next_token_id: {next_token_id}')

            if verbose:
                # Print the generated ablated text vs the original text at each step
                original_next_token_id = next_token_id[0]
                ablated_next_token_id = predicted_next_token_id[0]
                original_text = tokenizer.decode(original_next_token_id.to('cpu'), skip_special_tokens=True)
                ablated_text = tokenizer.decode(ablated_next_token_id.to('cpu'), skip_special_tokens=True)
                print(f"Original: \"{original_text}\", Ablated: \"{ablated_text}\"")

    # Convert logits lists to tensors
    ablated_logits = torch.stack(ablated_logits)

    # Decode the predicted tokens (from the ablated model's outputs) to text
    generated_text = tokenizer.decode(predicted_ids[0].to('cpu'), skip_special_tokens=True)

    return ablated_logits, divergence_list, generated_text


def sample_with_temperature(logits, temperature):
    """
    Samples from the logits with temperature.

    Args:
        logits: Logits tensor of shape [batch_size, vocab_size]
        temperature: Temperature value

    Returns:
        next_token_ids: Tensor of shape [batch_size, 1] containing the sampled token ids
    """
    if temperature == 0:
        # Deterministic: take the token with the highest probability
        next_token_ids = torch.argmax(logits, dim=-1).unsqueeze(-1)
    else:
        # Apply temperature
        logits = logits / temperature
        # Compute probabilities
        probabilities = F.softmax(logits, dim=-1)
        # Sample from the distribution
        next_token_ids = torch.multinomial(probabilities, num_samples=1)
    return next_token_ids

def generate_text_with_logits_batch(model, tokenizer, num_tokens_to_generate: int, device: str, prompts, temperature=0.0):
    """
    Autoregressively generates text from a batch of prompts while capturing the logits at each step.

    Args:
        model: Pre-trained transformer model.
        tokenizer: Corresponding tokenizer.
        num_tokens_to_generate: Number of tokens to generate.
        device: The device (e.g., 'cpu' or 'cuda') to run the generation on.
        prompts: List of input prompts for text generation.
        temperature: Sampling temperature, set to 0 for deterministic output (argmax).

    Returns:
        generated_texts: List of generated texts (excluding the prompt tokens).
        logits_list: Tensor of logits at each generation step (shape: [batch_size, num_steps, vocab_size]).
        generated_new_ids: Tensor of generated token ids (shape: [batch_size, num_tokens_to_generate]).
    """

    # Tokenize the batch of prompts
    # Check and set the padding token
    if tokenizer.pad_token is None:
        if tokenizer.eos_token:
            tokenizer.pad_token = tokenizer.eos_token
        else:
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    inputs = tokenizer(prompts, return_tensors='pt', padding=True)
    input_ids = inputs['input_ids'].to(device)  # shape [batch_size, seq_len]
    attention_mask = inputs['attention_mask'].to(device)  # shape [batch_size, seq_len]

    batch_size = input_ids.shape[0]

    # Initialize generated_ids with input_ids
    generated_ids = input_ids.clone()
    generated_ids = generated_ids.to(device)


    # Initialize lists to collect logits and newly generated token ids
    logits_list = []
    generated_new_ids = []

    for t in range(num_tokens_to_generate):
        # Show cuda memory usage
        print(f"Timestep: {t}, Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

        gc.collect()  # Explicitly invoke garbage collection
        with torch.no_grad():
            outputs = model(input_ids=generated_ids, attention_mask=attention_mask)

        # Logits for the next token predictions
        next_token_logits = outputs.logits[:, -1, :]  # shape [batch_size, vocab_size]

        # Record the logits
        logits_list.append(next_token_logits.detach().cpu())  # List of [batch_size, vocab_size]

        # Sample the next token or take the argmax for deterministic output
        next_token_ids = sample_with_temperature(next_token_logits, temperature=temperature)  # shape [batch_size, 1]

        # Concatenate the newly predicted token to the sequence
        generated_ids = torch.cat((generated_ids, next_token_ids), dim=1)  # shape [batch_size, seq_len + t + 1]

        # Update the attention mask
        attention_mask = torch.cat((attention_mask, torch.ones(batch_size, 1).to(device)), dim=1)

        # Collect the new ids
        generated_new_ids.append(next_token_ids.cpu())  # List of [batch_size, 1]

        # Clean GPU memory
        del outputs
        torch.cuda.empty_cache()

    # Stack logits_list to form a tensor of shape [batch_size, num_tokens_to_generate, vocab_size]
    logits_list = torch.stack(logits_list, dim=1)  # Now batch dimension is first

    # Concatenate generated_new_ids to form a tensor of shape [batch_size, num_tokens_to_generate]
    generated_new_ids = torch.cat(generated_new_ids, dim=1)  # shape [batch_size, num_tokens_to_generate]

    # Decode the generated ids to text
    generated_texts = [tokenizer.decode(generated_new_ids[i], skip_special_tokens=True) for i in range(batch_size)]

    return generated_texts, logits_list, generated_new_ids


def generate_with_teacher_forcing_ablated_batch(
    model, tokenizer, prompts, non_ablated_token_ids, non_ablated_logits, device: str,
    temperature=0.0, ablated_attention_heads=None, verbose=False
):
    # Move original tensors to the specified device
    non_ablated_token_ids = non_ablated_token_ids.to(device)
    non_ablated_logits = non_ablated_logits.to(device)

    # Tokenize the prompts and get prompt lengths
    if tokenizer.pad_token is None:
        if tokenizer.eos_token:
            tokenizer.pad_token = tokenizer.eos_token
        else:
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    # Tokenize the prompts with explicit padding and truncation
    tokenized_inputs = tokenizer(
        prompts,
        return_tensors='pt',
        padding=True,  # Ensure padding to the max length in the batch
        truncation=True,  # Truncate sequences exceeding the model's max length
        max_length=model.config.max_position_embeddings  # Use model's max length
    ).to(device)
    input_ids = tokenized_inputs['input_ids']  # [batch_size, prompt_length]
    attention_mask = tokenized_inputs['attention_mask']  # [batch_size, prompt_length]
    batch_size = input_ids.size(0)
    prompt_lengths = attention_mask.sum(dim=1)  # [batch_size]

    # Determine total sequence lengths from non_ablated_token_ids
    pad_token_id = tokenizer.pad_token_id
    generation_lengths = (non_ablated_token_ids != pad_token_id).sum(dim=1)  # [batch_size]
    max_generation_length = generation_lengths.max().item()

    if verbose:
        print(f"Batch size: {batch_size}")
        print(f"Prompt lengths: {prompt_lengths.tolist()}")
        print(f"Generation lengths: {generation_lengths.tolist()}")
        print(f"Max generation length: {max_generation_length}")

    # Initialize sequences
    ablated_ids = input_ids.clone().to(device)  # Start with the prompts
    predicted_ids = input_ids.clone().to(device)  # To store tokens predicted by the ablated model

    # Initialize containers for logits and divergences
    ablated_logits_list = []
    divergence_tensor = torch.zeros(batch_size, max_generation_length, device=device)

    for t in range(max_generation_length):
        gc.collect()  # Explicit garbage collection
        if t%10 == 0:
            print(f"Timestep: {t}, Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")


        with torch.no_grad():
            outputs = model(ablated_ids, ablated_attention_heads=ablated_attention_heads, attention_mask=attention_mask)
            ablated_next_token_logits = outputs.logits[:, -1, :]  # [batch_size, vocab_size]

            # Move everything to CPU for memory efficiency
            # ablated_next_token_logits = ablated_next_token_logits.cpu()
            # Delete outputs to free up memory
            del outputs


        # Record the ablated model's logits
        ablated_logits_list.append(ablated_next_token_logits.detach().cpu())
        if temperature == 0:
            predicted_next_token_id = torch.argmax(ablated_next_token_logits, dim=-1).unsqueeze(-1)
        else:
            predicted_next_token_id = sample_with_temperature(ablated_next_token_logits, temperature=temperature)

        # Append the predicted token to the predicted_ids sequence
        predicted_ids = torch.cat((predicted_ids, predicted_next_token_id), dim=1)

        # Calculate divergence between the original and ablated model's logits (KL divergence example)
        original_logit = non_ablated_logits[:, t, :]  # Align the logits with the generated tokens
        log_probs_ablated = F.log_softmax(ablated_next_token_logits, dim=-1)
        probs_original = F.softmax(original_logit, dim=-1)
        # Compute element-wise KL divergence without reduction
        elementwise_kl_div = F.kl_div(log_probs_ablated, probs_original, reduction='none')  # Shape: [3, 256000]

        # Sum the KL divergence over the vocabulary dimension to get per-sample divergence
        divergence_per_sample = elementwise_kl_div.sum(dim=-1)  # Shape: [3]

        # Store the divergence in the tensor
        divergence_tensor[:, t] = divergence_per_sample
        # print(f'Divergence_tensor: {divergence_tensor}') # [batch_size, max_generation_length]

        # Update the ablated_ids with the original model's generated token (Teacher forcing)
        next_token_id = non_ablated_token_ids[:, t].unsqueeze(-1).to(device)
        ablated_ids = torch.cat((ablated_ids, next_token_id), dim=1)

        # Append a '1' to attention_mask to reflect the new, valid token
        new_mask_token = torch.ones((attention_mask.size(0), 1), dtype=attention_mask.dtype, device=attention_mask.device)
        attention_mask = torch.cat((attention_mask, new_mask_token), dim=1)

    # Stack the logits list to form a tensor of shape [batch_size, num_tokens_to_generate, vocab_size]
    ablated_logits_tensor = torch.stack(ablated_logits_list, dim=1)

    # Decode the predicted tokens (from the ablated model's outputs) to text
    generated_texts = [tokenizer.decode(predicted_ids[i], skip_special_tokens=True) for i in range(batch_size)]


      
    return ablated_logits_tensor.cpu(), divergence_tensor.cpu(), generated_texts

import pickle as pkl
import constants

def load_ablations_results():
    file_path = constants.ABLATIONS_DIR + 'syn_minus_red/divergence_results.pkl'
    ablations_data = pkl.load(open(file_path, 'rb'))
    return ablations_data


def plot_kl_divergence(ablation_results, cognitive_task, prompt_number, save=False, save_dir=None, proportion_heads_shown=1, figsize=(14, 6)):
    """
    Plots KL divergence curves with confidence intervals for random ablations
    and the corresponding synergy-based ablation for a given cognitive task and prompt.

    Parameters:
    - ablation_results (dict): Dictionary containing ablation data.
    - cognitive_task (str): The cognitive task category to analyze (e.g., 'simple_maths').
    - prompt_number (int): The index of the prompt within the selected cognitive task.
    - save (bool): If True, saves the plot to the specified directory instead of displaying it.
    - save_dir (str): Directory path to save the plot (required if save=True).
    - num_total_heads (int, optional): If specified, only plot ablations for attention heads with index <= num_total_heads.

    Returns:
    - None: Displays or saves the plot.
    """
    # Load data for the specified cognitive task and prompt
    try:
        results = ablation_results['divergences'][cognitive_task][prompt_number]
        random_curves = np.array(results['random'])  # Random ablation curves
        synergy_curve = np.array(results['gradient'])  # Synergy-based ablation curve
    except KeyError:
        print(f"Error: Invalid cognitive task '{cognitive_task}' or prompt number '{prompt_number}'.")
        return
    except IndexError:
        print(f"Error: Prompt number '{prompt_number}' is out of range for task '{cognitive_task}'.")
        return

    # Ablated heads (x-axis)
    x = ablation_results['list_heads_ablated']

    # Filter data based on num_total_heads if specified
    num_total_heads = constants.NUM_TOTAL_HEADS * proportion_heads_shown
    mask = np.array(x) <= num_total_heads
    x = np.array(x)[mask]
    random_curves = random_curves[:, mask]
    synergy_curve = synergy_curve[mask]

    # Compute mean and confidence intervals for random ablations
    random_mean = np.mean(random_curves, axis=0)
    random_std = np.std(random_curves, axis=0)
    random_ci_upper = random_mean + 1.96 * (random_std / np.sqrt(random_curves.shape[0]))
    random_ci_lower = random_mean - 1.96 * (random_std / np.sqrt(random_curves.shape[0]))

    # Create the figure
    plt.figure(figsize=figsize)

    # Plot random ablations
    plt.plot(x, random_mean, label="Random Ablations (Mean)", color="orange")
    plt.fill_between(x, random_ci_lower, random_ci_upper, color="orange", alpha=0.3, label="95% CI (Random)")

    # Plot synergy-based ablation
    plt.plot(x, synergy_curve, label="Synergy-Based Ablation", color="blue")

    # Formatting
    plt.xlabel("Number of Ablated Heads")
    plt.ylabel("KL Divergence")
    plt.title(f"KL Divergence vs Number of Ablated Heads\nTask: {cognitive_task}, Prompt: {prompt_number}")
    plt.legend()
    plt.grid(True)

    if save:
        if save_dir is None:
            save_dir = constants.PLOT_ABLATIONS + 'syn_minus_red' + '/' +  '1-Divergence_Plots/' + cognitive_task + '/' 
        os.makedirs(save_dir, exist_ok=True)
        plt.savefig(save_dir + str(prompt_number) + '.png')
    else:
        plt.show()
    plt.close()

def plot_aggregated_kl_divergence(ablation_results, save=False, save_dir=None, proportion_heads_shown=1, prompt_category=None, 
                                  figsize=(14, 6), num_total_heads=constants.NUM_TOTAL_HEADS):
    """
    Plots aggregated KL divergence curves with confidence intervals for random ablations
    and the corresponding synergy-based ablation across all cognitive tasks and prompts.

    Parameters:
    - ablation_results (dict): Dictionary containing ablation data.
    - save (bool): If True, saves the plot to the specified directory instead of displaying it.
    - save_dir (str): Directory path to save the plot (required if save=True).
    - proportion_heads_shown (float): Proportion of total attention heads to include in the plot (0 < x <= 1).
    - prompt_category (Optional(str)): Cognitive task category along to which aggregate. If None, aggregated accross all categories.

    Returns:
    - None: Displays or saves the plot.
    """
    if prompt_category is not None:
        prompt_categories = [prompt_category]
    else:
        prompt_categories = constants.PROMPT_CATEGORIES

    # Initialize lists to aggregate all random and synergy results across prompts
    all_random_curves = []
    all_synergy_curves = []

    # Aggregate data over all specified prompt categories and prompts
    for category in prompt_categories:
        if category in ablation_results['divergences']:
            for prompt in ablation_results['divergences'][category].values():
                all_random_curves.extend(prompt['random'])  # Add all random ablation curves
                all_synergy_curves.append(prompt['gradient'])  # Add synergy ablation curves

    # Convert to numpy arrays for easier computation
    all_random_curves = np.array(all_random_curves)  # Shape (total_random_samples, num_points_on_curve)
    all_synergy_curves = np.array(all_synergy_curves)  # Shape (total_prompts, num_points_on_curve)

    # Ablated heads (x-axis)
    x = ablation_results['list_heads_ablated']

    # Filter data based on proportion_heads_shown
    num_total_heads = proportion_heads_shown * num_total_heads
    mask = np.array(x) <= num_total_heads
    x = np.array(x)[mask]
    all_random_curves = all_random_curves[:, mask]
    all_synergy_curves = all_synergy_curves[:, mask]

    # Compute mean and confidence intervals for aggregated random ablations
    random_mean_agg = np.mean(all_random_curves, axis=0)
    random_std_agg = np.std(all_random_curves, axis=0)
    random_ci_upper_agg = random_mean_agg + 1.96 * (random_std_agg / np.sqrt(all_random_curves.shape[0]))
    random_ci_lower_agg = random_mean_agg - 1.96 * (random_std_agg / np.sqrt(all_random_curves.shape[0]))

    # Compute mean for aggregated synergy-based ablations (since there's one per prompt)
    synergy_mean_agg = np.mean(all_synergy_curves, axis=0)
    synergy_std_agg = np.std(all_synergy_curves, axis=0)
    synergy_ci_upper_agg = synergy_mean_agg + 1.96 * (synergy_std_agg / np.sqrt(all_synergy_curves.shape[0]))
    synergy_ci_lower_agg = synergy_mean_agg - 1.96 * (synergy_std_agg / np.sqrt(all_synergy_curves.shape[0]))

    # Create the aggregated figure
    plt.figure(figsize=figsize)

    # Plot aggregated random ablations
    plt.plot(x, random_mean_agg, label="Random Ablations", color="orange")
    plt.fill_between(x, random_ci_lower_agg, random_ci_upper_agg, color="orange", alpha=0.3)#, label="95% CI (Random)")

    # Plot aggregated synergy-based ablations
    plt.plot(x, synergy_mean_agg, label="Synergy-minus-Redundancy Based Ablations", color="blue")
    plt.fill_between(x, synergy_ci_lower_agg, synergy_ci_upper_agg, color="blue", alpha=0.3)#, label="95% CI (Synergy)")

    # Formatting
    plt.xlabel("Number of Ablated Heads")
    plt.ylabel("KL Divergence")
    plt.title("Aggregated KL Divergence Across All Prompts")
    plt.legend()
    plt.grid(True)

    if save:
        if save_dir is None:
            save_dir = constants.PLOT_ABLATIONS + 'syn_minus_red/' + '1-Divergence_Plots/Aggregated/'
        os.makedirs(save_dir, exist_ok=True)
        title = 'all' if prompt_category is None else prompt_category
        plt.savefig(save_dir + title + '.png')
    else:
        plt.show()
    plt.close()